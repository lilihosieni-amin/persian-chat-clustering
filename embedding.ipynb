{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olmjexNB3P5U",
        "outputId": "ad77ed98-b684-4eaa-e312-914c2fd04b8c",
        "collapsed": true
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np # type: ignore\n",
        "import time\n",
        "import os\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm # type: ignore\n",
        "from openai import OpenAI # type: ignore\n",
        "\n",
        "client = OpenAI(api_key=\"\")\n",
        "\n",
        "input_csv = \"/content/drive/MyDrive/Doukt/Goftino/NLP/colab/embedd/preproces.csv\"\n",
        "output_csv = \"/content/drive/MyDrive/Doukt/Goftino/NLP/colab/embedd/embeddings.csv\"\n",
        "output_parquet = \"/content/drive/MyDrive/Doukt/Goftino/NLP/colab/embedd/embeddings.parquet\"\n",
        "\n",
        "# Set chunk size for better performance\n",
        "chunksize = 1000\n",
        "first_chunk = True\n",
        "chunk_counter = 0\n",
        "total_processed = 0\n",
        "\n",
        "# Number of concurrent requests\n",
        "max_workers = 5\n",
        "\n",
        "# Remove output files if they already exist\n",
        "if os.path.exists(output_csv):\n",
        "    os.remove(output_csv)\n",
        "\n",
        "print(f\"Starting to process file {input_csv} with chunk size {chunksize} and {max_workers} concurrent requests...\")\n",
        "\n",
        "def get_embedding(text_data, max_retries=3, base_delay=1):\n",
        "    text, idx, original_idx = text_data\n",
        "    retries = 0\n",
        "\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            if pd.isna(text) or text.strip() == \"\":\n",
        "                return None, idx, original_idx\n",
        "\n",
        "            response = client.embeddings.create(\n",
        "                model=\"text-embedding-3-large\",\n",
        "                input=text\n",
        "            )\n",
        "\n",
        "            return response.data[0].embedding, idx, original_idx\n",
        "\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            error_msg = str(e).lower()\n",
        "\n",
        "            if \"rate_limit\" in error_msg:\n",
        "                delay = 30\n",
        "            else:\n",
        "                delay = base_delay * (2 ** (retries - 1))\n",
        "\n",
        "            if retries < max_retries:\n",
        "                print(f\"Error at index {original_idx} (attempt {retries} of {max_retries}): {e}\")\n",
        "                print(f\"Retrying after {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Final error at index {original_idx} after {max_retries} attempts: {e}\")\n",
        "                return None, idx, original_idx\n",
        "\n",
        "    return None, idx, original_idx\n",
        "\n",
        "# Keep track of the total number of rows processed across all chunks\n",
        "base_index = 0\n",
        "\n",
        "for chunk in pd.read_csv(input_csv, chunksize=chunksize):\n",
        "    chunk_counter += 1\n",
        "    if \"preprocessed\" not in chunk.columns:\n",
        "        print(\"'preprocessed' column not found.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Prepare data for parallel processing with original index\n",
        "    # For each row in the chunk, store: (preprocessed_text, position_in_chunk, original_position_in_csv)\n",
        "    texts_with_indices = []\n",
        "    for i, row in chunk.iterrows():\n",
        "        # Calculate original index in the whole CSV\n",
        "        original_idx = base_index + (i - chunk.index[0])\n",
        "        texts_with_indices.append((row[\"preprocessed\"], i, original_idx))\n",
        "\n",
        "    results = [None] * len(texts_with_indices)\n",
        "    original_indices_for_results = [None] * len(texts_with_indices)\n",
        "\n",
        "    print(f\"Processing chunk {chunk_counter} with {len(texts_with_indices)} records...\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_idx = {executor.submit(get_embedding, item): i for i, item in enumerate(texts_with_indices)}\n",
        "\n",
        "        for future in tqdm(concurrent.futures.as_completed(future_to_idx), total=len(texts_with_indices), desc=f\"Processing chunk {chunk_counter}\"):\n",
        "            idx = future_to_idx[future]\n",
        "            try:\n",
        "                embedding, _, original_idx = future.result()\n",
        "                if embedding is not None:\n",
        "                    results[idx] = embedding\n",
        "                    original_indices_for_results[idx] = original_idx\n",
        "            except Exception as e:\n",
        "                print(f\"Error while processing: {e}\")\n",
        "\n",
        "    # Remove None results while keeping original indices\n",
        "    valid_results = [(emb, idx) for emb, idx in zip(results, original_indices_for_results) if emb is not None]\n",
        "    if valid_results:\n",
        "        embeddings, indices = zip(*valid_results)\n",
        "\n",
        "        # Convert list of embeddings to DataFrame with numeric columns\n",
        "        arr = np.array(embeddings, dtype=np.float32)\n",
        "        df_emb = pd.DataFrame(arr, columns=[f\"dim_{i}\" for i in range(arr.shape[1])])\n",
        "\n",
        "        # Add original index column\n",
        "        df_emb.insert(0, 'original_index', indices)\n",
        "\n",
        "        # Save to CSV file (append mode)\n",
        "        if first_chunk:\n",
        "            df_emb.to_csv(output_csv, index=False, mode='w')\n",
        "            first_chunk = False\n",
        "        else:\n",
        "            df_emb.to_csv(output_csv, index=False, mode='a', header=False)\n",
        "\n",
        "        total_processed += len(embeddings)\n",
        "        print(f\"✅ Chunk {chunk_counter} with {len(embeddings)} records processed and appended to CSV\")\n",
        "\n",
        "        # Free memory\n",
        "        del arr, df_emb, embeddings, indices, valid_results\n",
        "\n",
        "    # Update the base_index for the next chunk\n",
        "    # We add the length of the current chunk to our running total\n",
        "    base_index += len(chunk)\n",
        "\n",
        "print(f\"\\nProcessing of {total_processed} records in {chunk_counter} chunks completed\")\n",
        "print(f\"CSV file saved at {output_csv}\")\n",
        "\n",
        "# Convert CSV file to Parquet\n",
        "print(\"\\nConverting CSV to Parquet...\")\n",
        "\n",
        "# Read CSV in small chunks and convert to Parquet\n",
        "csv_chunksize = 5000\n",
        "first_parquet_chunk = True\n",
        "\n",
        "for csv_chunk in tqdm(pd.read_csv(output_csv, chunksize=csv_chunksize), desc=\"Converting to Parquet\"):\n",
        "    if first_parquet_chunk:\n",
        "        csv_chunk.to_parquet(output_parquet, compression=\"snappy\", index=False)\n",
        "        first_parquet_chunk = False\n",
        "    else:\n",
        "        # For simplicity and reliability, we'll use the append method with a temporary file\n",
        "        temp_parquet = output_parquet + \".temp\"\n",
        "\n",
        "        # Read the existing parquet\n",
        "        existing_df = pd.read_parquet(output_parquet)\n",
        "\n",
        "        # Combine with new chunk\n",
        "        combined_df = pd.concat([existing_df, csv_chunk], ignore_index=True)\n",
        "\n",
        "        # Save to temp file\n",
        "        combined_df.to_parquet(temp_parquet, compression=\"snappy\", index=False)\n",
        "\n",
        "        # Replace original with temp\n",
        "        os.replace(temp_parquet, output_parquet)\n",
        "\n",
        "        # Clean up\n",
        "        del existing_df, combined_df\n",
        "\n",
        "print(f\"✅ Successfully converted to Parquet. Final file saved at {output_parquet}\")\n",
        "print(f\"\\n✅ All done! {total_processed} records processed. Both CSV and Parquet files saved:\")\n",
        "print(f\"- CSV file: {output_csv}\")\n",
        "print(f\"- Parquet file: {output_parquet}\")\n"
      ]
    }
  ]
}
